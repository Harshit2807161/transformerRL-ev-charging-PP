# Optimizing In-Home EV Charging with Transformers and Policy-Based DRL

## Abstract

Managing EV charging is challenging due to factors like limited battery capacity, unpredictable user behavior, and fluctuating electricity prices. This project focuses on optimizing in-home EV charging using a deep reinforcement learning (DRL)-based Markov decision process (MDP) framework integrated with transformer-based models for forecasting.

We propose a time-series transformer-based network for electricity price forecasting, enhancing the scheduling of EV charging to minimize costs and maximize user satisfaction. Our transformer-based feature extraction model investigates historical price patterns over different time frames. DRL techniques like Deep Q-Networks (DQN), Deep Deterministic Policy Gradient (DDPG), and Proximal Policy Optimization (PPO) are used for decision-making, and we compare the results using three transformers: Autoformer, Informer, and PatchTST.

Our approach significantly reduces EV charging costs while maintaining high user satisfaction, outperforming prior models. The results show a 125.74% reduction in costs for continuous action space and 140.66% in discrete action space.

## Key Features

- **Deep Reinforcement Learning (DRL) Models:** Includes DQN, DDPG, and PPO models in both continuous and discrete forms.
- **Transformer Models:** Utilizes transformer-based models (Autoformer, Informer, PatchTST) to capture electricity price patterns and improve forecasts.
- **Time-series forecasting:** Employs different time-frames (past 24 hours, 24 days, and 24 weeks) for forecasting electricity prices.
- **Decision-making Models:** The decision framework determines optimal charging or discharging actions based on the predicted electricity price and the state of charge (SoC).

## Repository Structure

- **data/**: Contains CSV files (`full.csv`, `test.csv`, `train.csv`, `val.csv`) used for training, validation, and testing models. 
  
- **evc/**: 
  - `core.py`: Defines the environment logic.
  - `drl.py`: Implements the agent class and training procedures.
  - `db2_transformers.py`: Fetches price_predictor class for predicting transformer predictions using model_prediction csv files. 
  - `model_predictions/`: Stores model predictions generated by the transformer for various configurations in csv files. This is crucial for the RL environment. 
  - Other scripts are helpers for environment setup and database-related tasks.
  
- **main_{agent}.py**: (e.g., `main_dqn.py`, `main_ppo_disc.py`) CLI entry points to run the full reinforcement learning training and testing processes using different agent types (DQN, PPO, etc.).

- **rl.py**: The `__main__` script that coordinates agent training based on the provided command-line arguments.

- **requirements.txt**: Lists the dependencies required to run the project.

- **README.md**: Documentation of the project.


## Installation

To run the project, clone the repository and install the necessary dependencies:

```bash
git clone https://github.com/Harshit2807161/transformerRL-ev-charging-PP.git
cd transformerRL-ev-charging-PP
pip install -r requirements.txt
```

Dependencies can be found in the `requirements.txt` file, and they include popular machine learning libraries such as TensorFlow or PyTorch, transformers, and DRL frameworks.

## Usage

### 1. Data Preparation
The data is already prepared for training! 
The data files are placed in the `data/` directory. The dataset used in this project consists of historical price data for different time periods, including:

- Past 24 hours
- Same hour over the last 24 days
- Same hour on the same weekday over the last 24 weeks

### 2. Model Training

#### Transformer Model Training
Run the following script to train the transformer-based models (Autoformer, Informer, PatchTST) using Deep-RL agents (this code assumes transformer output is stored in a csv file). You may use our data or preprocess the data according to our framework using your own time-series forecasting model.

```bash
python rl.py --rla_names={agent} --epochs=210000 --do_train=1 --do_vis=1
```

where agent can be one of- 
```
dqn_01
ddpg_01
ppo_disc_01
ppo_cont_01
```

You may change the other input arguments by referring to `rl.py`. Another way to run the RL training without cli is to directly run `main_*.py` files in the repo for any agent. 

## License

This project is licensed under the MIT License. See the `LICENSE` file for more details.

---
